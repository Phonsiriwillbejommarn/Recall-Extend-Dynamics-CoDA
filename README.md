# ğŸ§  Recall-Extend Dynamics CoDA (RED-CoDA)

**Enhancing CoDA (Context-Decoupled Hierarchical Agent) with RED (Recall-Extend Dynamics) to train small language models as effective retrieval-augmented reasoning agents.**

Built on [CoDA](https://arxiv.org/abs/2505.xxxxx) + [RED](https://arxiv.org/abs/2505.xxxxx) frameworks, using Gemma-2-2B with GRPO reinforcement learning.

[![Model on HF](https://img.shields.io/badge/ğŸ¤—-Model-yellow)](https://huggingface.co/Phonsiri/CoDA-Gemma2-RED-v3)
[![W&B Dashboard](https://img.shields.io/badge/W%26B-Dashboard-blue)](https://wandb.ai)

---

## ğŸ“Œ Overview

Small Language Models (SLMs) struggle with complex multi-hop QA tasks that require retrieval. Standard approaches either:
- **SFT only** â†’ overfits to teacher patterns, poor generalization
- **RL only** â†’ insufficient exploration space for small models
- **SFT â†’ RL** â†’ catastrophic forgetting of learned patterns

**RED-CoDA** solves this by **jointly training SFT + RL** with dynamic weighting controlled by two mechanisms:

| RED Component | What it does |
|---|---|
| **Part 1: Dynamic Entropy Regulation** | Monitors entropy changes to balance exploration (RL) vs exploitation (SFT) |
| **Part 2: Accuracy-Aware Policy Shift** | When model answers poorly â†’ more SFT; when it answers well â†’ more RL |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Gemma-2-2B (Single LLM)         â”‚
â”‚                                              â”‚
â”‚   ğŸ§  Planner          âš¡ Executor            â”‚
â”‚   (Strategic)         (Ephemeral)            â”‚
â”‚   Plans long-term     Executes subtasks      â”‚
â”‚   Keeps context       Forgets after done     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â–¼                       â–¼
  search(query)          answer(result)
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ FAISS Index â”‚ â† Wikipedia (21M docs)
  â”‚ (CPU)       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RED Training Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1 Training Step                                     â”‚
â”‚                                                      â”‚
â”‚  ğŸ² RL Rollout (vLLM) â†’ Generate responses           â”‚
â”‚  ğŸ“Š Compute Reward (F1 + format + refine)            â”‚
â”‚  ğŸ“ˆ GRPO Advantage (group normalization)             â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€ RED Weight Computation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Part 1: entropy_weight = f(Î´H_sft / Î´H_rl)  â”‚   â”‚
â”‚  â”‚  Part 2: accuracy_factor = G^(1 - 2Â·acc)     â”‚   â”‚
â”‚  â”‚  final_weight = entropy Ã— accuracy            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                      â”‚
â”‚  ğŸ§  Actor Update:                                    â”‚
â”‚     RL loss (policy gradient)                        â”‚
â”‚     + final_weight Ã— SFT loss (cross-entropy)        â”‚
â”‚     â†’ single optimizer step                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composite Reward

| Component | Weight | Description |
|-----------|--------|-------------|
| **Answer Quality** | `6 Ã— F1 - 3` | F1 score vs ground truth (dominant) |
| **Format Compliance** | `0.1 Ã— score` | Graduated XML tag scoring (0.25/tag) |
| **Refinement Quality** | `0.1 Ã— score` | Search summarization quality |

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- CUDA 12.x compatible GPU (H100 recommended)
- ~140GB disk space (for retriever index + Wikipedia corpus)

### 1. Clone & Install

```bash
git clone https://github.com/Phonsiriwillbejommarn/Recall-Extend-Dynamics-CoDA.git
cd Recall-Extend-Dynamics-CoDA
pip install -e .
```

### 2. Login Services

```bash
wandb login            # For training dashboard
huggingface-cli login  # For checkpoint push
```

### 3. Download Data

```bash
# Download retriever index + Wikipedia corpus (~130GB)
bash preprocess/download_and_process.sh

# Process training data (NQ, HotpotQA, etc.)
bash preprocess/scripts/data_process.sh

# Generate SFT training data
python cmd/generate_sft_data.py
```

### 4. Start Training

```bash
# Terminal 1: Start Retrieval Server
bash retrieval_launch.sh

# Terminal 2: Start Training
bash cmd/train.sh
```

---

## âš™ï¸ Configuration

All configs in [`cmd/train.sh`](cmd/train.sh):

### Core Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `train_batch_size` | 32 | Prompts per training step |
| `n_agent` | 2 | Responses per prompt (GRPO group size) |
| `max_turns` | 2 | Search rounds per sample |
| `total_training_steps` | 480 | Total training steps |
| `learning_rate` | 1e-6 | Actor learning rate |

### RED Configuration

| Parameter | Default | Description |
|-----------|---------|-------------|
| `sft.enabled` | true | Enable SFT co-training |
| `sft.loss_coef` | 0.1 | Base SFT loss coefficient |
| `red.G` | 5.0 | Upper bound for RED weight |
| `red.sft_entropy_ema_decay` | 0.99 | SFT entropy EMA smoothing |
| `red.rl_entropy_ema_decay` | 0.99 | RL entropy EMA smoothing |
| `algorithm.accuracy_aware_policy_shift` | true | Enable Part 2 |

### Ablation Configurations

```bash
# Run 1: GRPO only (baseline)
sft.enabled=false

# Run 2: GRPO + fixed SFT (no dynamic weighting)
sft.enabled=true red.G=1.0

# Run 3: GRPO + RED (full)
sft.enabled=true red.G=5.0 algorithm.accuracy_aware_policy_shift=true
```

---

## ğŸ“ Project Structure

```
CoDA/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ train.sh                 # Main training script & config
â”‚   â”œâ”€â”€ auto_resume.py           # Auto-resume from HF Hub checkpoints
â”‚   â””â”€â”€ generate_sft_data.py     # Generate SFT training data
â”œâ”€â”€ search_r1/
â”‚   â”œâ”€â”€ llm_agent/
â”‚   â”‚   â””â”€â”€ generation.py        # Hierarchical agent (Planner/Executor)
â”‚   â””â”€â”€ search/
â”‚       â””â”€â”€ retrieval_server.py  # FastAPI retrieval server (FAISS)
â”œâ”€â”€ verl/
â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â”œâ”€â”€ main_ppo.py          # Entry point + RewardManager
â”‚   â”‚   â””â”€â”€ ppo/
â”‚   â”‚       â”œâ”€â”€ ray_trainer.py   # Training loop + RED integration
â”‚   â”‚       â””â”€â”€ core_algos.py    # GRPO + RED algorithms
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â””â”€â”€ actor/
â”‚   â”‚       â””â”€â”€ dp_actor.py      # Actor update (RL + SFT loss)
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ reward_score/
â”‚       â”‚   â””â”€â”€ qa_em.py         # Reward functions (F1, EM, format)
â”‚       â””â”€â”€ dataset/
â”‚           â”œâ”€â”€ rl_dataset.py    # RL training dataset
â”‚           â””â”€â”€ sft_dataset.py   # SFT co-training dataset
â”œâ”€â”€ data/                        # Training data (generated)
â””â”€â”€ requirements.txt
```

---

## ğŸ“Š W&B Metrics

### Key Metrics to Monitor

| Metric | Description |
|--------|-------------|
| `critic/rewards/mean` | Overall reward per step |
| `answer_f1/mean` | Answer quality (F1 score) |
| `format_scores/mean` | XML format compliance |
| `red/entropy_weight` | RED Part 1 â€” entropy-based weight |
| `red/accuracy_factor` | RED Part 2 â€” accuracy-based multiplier |
| `red/final_weight` | Combined RED weight |
| `red/batch_accuracy` | Fraction of correct answers |

---

## ğŸ”§ Restart After Server Reboot

```bash
cd Recall-Extend-Dynamics-CoDA
git pull origin main
bash preprocess/scripts/data_process.sh    # Recreate parquet files
python cmd/generate_sft_data.py            # Recreate SFT data
bash retrieval_launch.sh &                 # Start retriever
bash cmd/train.sh                          # Auto-resumes from HF Hub
```

> **Note:** If `wiki-18.jsonl` and `e5_Flat.index` are also missing, run `bash preprocess/download_and_process.sh` first.

---

## ğŸ“ License

Apache License 2.0

## ğŸ™ Acknowledgments

- Based on [CoDA](https://github.com/xxx/CoDA) â€” Context-Decoupled Hierarchical Agent
- RED framework adapted from [RED](https://arxiv.org/abs/2505.xxxxx) â€” Recall-Extend Dynamics
- Built on [Search-R1](https://github.com/PeterGriffinJin/Search-R1) and [verl](https://github.com/volcengine/verl)
- Model: [Google Gemma-2-2B](https://huggingface.co/google/gemma-2-2b)